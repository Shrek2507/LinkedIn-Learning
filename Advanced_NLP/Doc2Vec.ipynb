{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics Of Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messagesData = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "messagesData = messagesData.drop(['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Of the dataset is :  (5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Shape Of the dataset is : \", messagesData.shape)\n",
    "messagesData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Spam and Ham in the dataset are : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distribution of Spam and Ham in the dataset are : \")\n",
    "messagesData['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning\n",
    "\n",
    "Clean the text and pre-process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messagesData['text_clean'] = messagesData['text'].apply(lambda x: simple_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, so, early, hor, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...  \n",
       "1                                                                          [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...  \n",
       "3                                                       [dun, say, so, early, hor, already, then, say]  \n",
       "4                                [nah, don, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messagesData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test...\n",
    "X_train, X_test, y_train, y_test = train_test_split(messagesData['text_clean'], \n",
    "                                                    messagesData['label'], \n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tagged documents...\n",
    "tagged_docs = [TaggedDocument(words=value, tags=[str(index)]) for index, value in enumerate(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['gud', 'ni', 'dear', 'slp', 'well', 'take', 'care', 'swt', 'dreams', 'muah'], tags=['0'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v = Doc2Vec(tagged_docs,\n",
    "                   vector_size=100,\n",
    "                   window=5,\n",
    "                   min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.15543732e-03,  7.02834735e-03, -2.35414365e-03,  4.24405513e-03,\n",
       "        3.08401370e-03, -1.07090292e-03,  1.91943184e-03,  4.42718435e-03,\n",
       "       -7.00676395e-03,  4.38497495e-03,  1.56381782e-02,  4.93112020e-04,\n",
       "        6.60312548e-03,  9.75890644e-03, -3.62993637e-03,  1.44379528e-03,\n",
       "       -1.03157246e-02,  1.20594315e-02,  2.40801671e-03, -3.29758483e-03,\n",
       "        5.55190444e-03,  1.24145678e-04,  2.64947908e-03,  3.12144373e-04,\n",
       "       -5.47410338e-04,  3.67438816e-03, -2.14254507e-03,  1.75181404e-03,\n",
       "       -7.45737180e-03,  2.03588349e-03, -1.14017814e-04,  1.07255857e-03,\n",
       "        9.10566375e-03, -3.80413351e-03, -5.10922819e-03, -3.99098685e-03,\n",
       "        2.09977245e-03,  2.85463524e-03, -7.01277424e-03,  8.19729734e-03,\n",
       "       -1.49877975e-03, -2.91218981e-03,  2.70821969e-03, -1.53051992e-03,\n",
       "       -6.75039273e-03, -3.83164757e-03, -1.96160702e-03, -1.49537373e-04,\n",
       "        3.93639645e-03,  3.77238076e-03,  1.74163142e-04,  9.65790171e-03,\n",
       "       -1.02547063e-02,  7.16797134e-04,  4.41203127e-03, -6.77914405e-03,\n",
       "        2.54817400e-03, -4.79375012e-03, -7.58070219e-03, -1.10761460e-03,\n",
       "        1.42020150e-03,  9.73966252e-03,  8.54442175e-03, -9.27204266e-04,\n",
       "        3.33196553e-03,  4.82586725e-03, -1.03558572e-02, -1.65204878e-03,\n",
       "       -8.26159213e-03, -3.40687507e-03,  6.76535629e-03, -6.72544411e-04,\n",
       "       -7.90921226e-03, -2.09404016e-03,  1.00500518e-02, -1.38993247e-03,\n",
       "       -4.89403959e-03,  2.76938168e-04, -1.11000612e-03, -5.99406147e-03,\n",
       "       -3.07910424e-03,  1.58925867e-03,  2.90867465e-04, -3.93682206e-03,\n",
       "        2.19317456e-03,  1.83764449e-03,  2.96340976e-03, -9.16238350e-05,\n",
       "       -5.98298525e-03, -9.67708940e-04, -2.64556822e-03, -1.91985699e-03,\n",
       "        1.51023181e-04, -2.47441046e-03,  3.37740756e-03,  8.58397502e-03,\n",
       "       -5.28334174e-03, -1.04148611e-02, -4.43697674e-03, -1.20120868e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d2v.infer_vector(['i','am', 'learning', 'nlp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the vectors to be passed into the machine learning model...\n",
    "vectors = [[model_d2v.infer_vector(word)] for word in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-5.3671678e-03,  5.1020185e-04, -4.0908796e-03,  9.0921402e-04,\n",
       "         4.0739886e-03, -4.7100205e-03, -3.7805827e-03,  3.4590655e-03,\n",
       "        -1.8986421e-02,  9.7225793e-03,  1.0686611e-02,  3.4462474e-03,\n",
       "         1.2066212e-03,  4.6618679e-03,  1.5893063e-03, -2.2070017e-03,\n",
       "        -1.0551234e-02,  1.2079314e-02, -1.3269404e-03,  1.7862121e-03,\n",
       "         4.4846102e-03, -9.2857977e-04,  3.2806767e-03, -1.8103214e-03,\n",
       "         5.8145646e-04,  5.4084095e-03, -7.1187275e-03,  7.4259588e-03,\n",
       "        -1.9709599e-04,  4.4738976e-03, -1.8609380e-03, -4.1548279e-03,\n",
       "         6.9660009e-03, -1.2002401e-02,  1.5237051e-03, -6.7705852e-03,\n",
       "         9.4192292e-05,  4.8808581e-03, -4.0833391e-03,  4.8780679e-03,\n",
       "        -4.7803614e-03, -7.4840141e-03,  2.7903200e-03, -5.0829852e-04,\n",
       "        -4.9716174e-03, -8.1258826e-03,  5.1625711e-03, -4.2339740e-03,\n",
       "         8.4361713e-03,  7.5110891e-03,  6.0809311e-04,  1.2092414e-02,\n",
       "        -5.0695315e-03, -5.4128296e-03,  3.5129662e-03, -1.0881912e-03,\n",
       "        -4.7243270e-03, -5.6724525e-03, -3.9322185e-03,  2.0776393e-03,\n",
       "         3.0017996e-03,  1.0957311e-02,  8.0587454e-03,  4.4111532e-04,\n",
       "        -1.5409684e-03, -1.1901124e-04, -1.4233231e-02,  4.5243967e-03,\n",
       "        -1.0312588e-02, -4.0063462e-03,  6.1153308e-03,  3.2624668e-03,\n",
       "        -6.0559013e-03, -7.2680553e-04,  5.3754277e-03,  4.8121158e-03,\n",
       "        -1.1709991e-02,  7.4542612e-03, -2.2378606e-03, -6.4829453e-03,\n",
       "         1.8186459e-03,  1.0034008e-03, -8.2383439e-04, -1.5493747e-03,\n",
       "         1.8132221e-03,  8.2211345e-03, -4.8229620e-03, -1.9665100e-03,\n",
       "        -2.2250910e-03,  6.1403108e-03,  1.6721827e-03,  1.5201825e-03,\n",
       "         2.0301410e-03, -9.0311812e-03, -2.9448983e-03,  3.9507318e-03,\n",
       "        -2.8846241e-03, -1.2704062e-02, -2.7242226e-03,  5.5765351e-03],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
